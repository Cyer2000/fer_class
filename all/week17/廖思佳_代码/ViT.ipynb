{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange,repeat #rearrang 可以修改张量的维数\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair函数\n",
    "* 判断t是否是元组，如果是，直接返回t；如果不是，则将t复制为元组(t, t)再返回。\n",
    "用来处理当给出的图像尺寸或块尺寸是int类型（如224）时，直接返回为同值元组（如(224, 224)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair(t):\n",
    "    return t if isinstance(t,tuple) else (t,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreNorm\n",
    "* 对应框图中最下面的黄色的Norm层\n",
    "* 其参数dim是维度，而fn则是预先要进行的处理函数，是以下的Attention、FeedForward之一\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self,dim,fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self,x,**kwargs):\n",
    "        return self.fn(self.norm(x),**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),  # 激活函数\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head**-0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(\n",
    "            3, dim=-1)  # (b, n(65), dim*3) ---> 3 * (b, n, dim)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "* depth 是每个transformer block重复的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(\n",
    "                nn.ModuleList([\n",
    "                    PreNorm(dim,Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                    PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "                ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool='cls',channels=3,dim_head=64, dropout=0.,emb_dropout=0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'} \n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_height,p2=patch_width),\n",
    "            nn.Linear(patch_dim, dim)\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,num_patches+1,dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.tansformer = Transformer(dim, depth, heads,dim_head,mlp_dim,dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latend = nn.Identity()  # 不改变输入 增减网络的过程中我们就可以用identity占个位置，这样网络整体层数永远不变\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self,img):\n",
    "        x =  self.to_patch_embedding(img)\n",
    "        b,n,_ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d',b=b)\n",
    "        x =torch.cat((cls_tokens,x),dim = 1)\n",
    "        x += self.pos_embedding[:,:(n+1)] # 加位置嵌入\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:,0]\n",
    "        \n",
    "        x = self.to_latend(x)\n",
    "        print(x.shape)\n",
    "\n",
    "        return self.mlp_head(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理数据集RFDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAFDB(Dataset):\n",
    "    labels_num2str = ['Surprise', 'Fear', 'Disgust', 'Happiness', 'Sadness', 'Neutral']\n",
    "    labels_str2num = {v:k for k, v in enumerate(labels_num2str)}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        path:str,\n",
    "        mode:str,\n",
    "        transform = transforms.Compose(\n",
    "        (\n",
    "            transforms.Resize((100, 100)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                 mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        )\n",
    "    )):\n",
    "\n",
    "        self._label_map = dict()\n",
    "        with open(os.path.join(path, 'EmoLabel', 'list_patition_label.txt')) as f:\n",
    "            for line in f.read().splitlines():\n",
    "                filename, label = line.split(' ')\n",
    "                label = int(label) -1\n",
    "                self._label_map[filename.split('.')[0]] =label\n",
    "\n",
    "        img_dir = os.path.join(path,'Image', 'aligned')\n",
    "        self._image_paths = []\n",
    "        for image_name in os.listdir(img_dir):\n",
    "            if image_name.startswith(mode):\n",
    "                self._image_paths.append(os.path.join(img_dir, image_name))\n",
    "\n",
    "        self._transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self._image_paths[idx]\n",
    "        image_name = \"_\".join(os.path.split(image_path)[1].split(\"_\")[:2])\n",
    "        label = self._label_map[image_name]\n",
    "        data = Image.open(image_path)\n",
    "        data = self._transform(data)\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1024])\n",
      "torch.Size([16, 1000])\n"
     ]
    }
   ],
   "source": [
    "model_vit = ViT(\n",
    "        image_size = 256,\n",
    "        patch_size = 32,\n",
    "        num_classes = 1000,\n",
    "        dim = 1024,\n",
    "        depth = 6,\n",
    "        heads = 16,\n",
    "        mlp_dim = 2048,\n",
    "        dropout = 0.1,\n",
    "        emb_dropout = 0.1\n",
    "    )\n",
    "\n",
    "img = torch.randn(16, 3, 256, 256)\n",
    "\n",
    "preds = model_vit(img) \n",
    "\n",
    "print(preds.shape)  # (16, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"E:\\JuniorYear\\大创\\王艳\\数据集\\raf-basic\"\n",
    "train_set = RAFDB(dataset_path, \"train\")\n",
    "test_set = RAFDB(dataset_path, \"test\")\n",
    "device = \"cuda\"\n",
    "\n",
    "model = ViT(\n",
    "    image_size=100,\n",
    "    patch_size=5,\n",
    "    num_classes=10,\n",
    "    dim=64,\n",
    "    depth=6,\n",
    "    heads=8,\n",
    "    mlp_dim=256,\n",
    "    dropout=0.1,\n",
    "    emb_dropout=0.1,\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "test_batch_size = 32\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# for every step, this scheduler times the lr by gamma\n",
    "# step_size if how many steps before it times the lr by gamma\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True, num_workers=4\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=test_batch_size, shuffle=True, num_workers=4\n",
    ")\n",
    "\n",
    "test_results = []\n",
    "model.to(device)\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x.to(device))\n",
    "        loss = criterion(output, y.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                f\"epoch: {epoch}, i: {batch_idx*len(x)}/{len(train_loader.dataset)}, loss: {loss.item()}\"\n",
    "            )\n",
    "\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    correct_count = 0\n",
    "    for batch_idx, (x, y) in enumerate(test_loader):\n",
    "        output = model(x.to(device))\n",
    "        correct_count += output.argmax(dim=-1).eq(y.to(device)).count_nonzero()\n",
    "    print(\n",
    "        f\"correct: {correct_count}/{len(test_loader.dataset)}, {correct_count/len(test_loader.dataset)*100:.2f}%\"\n",
    "    )\n",
    "    test_results.append(float(correct_count/len(test_loader.dataset)))\n",
    "\n",
    "print(f\"test_results: {test_results}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be203ce0b3afc4f5c37fbac412025d7ed1d67cabe9dd00b1fc8774c6d6d19d70"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
