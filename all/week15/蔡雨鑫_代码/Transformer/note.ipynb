{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import things\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import einops\n",
    "\n",
    "from torch import nn\n",
    "from IPython.display import display\n",
    "\n",
    "pil = torchvision.transforms.ToPILImage()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying to use einops to split image into patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to dataset\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset\\MNIST\\raw\\train-images-idx3-ubyte.gz to dataset\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to dataset\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset\\MNIST\\raw\\train-labels-idx1-ubyte.gz to dataset\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to dataset\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to dataset\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to dataset\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%\n",
      "D:\\annaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to dataset\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1ElEQVR4nGNgGArA+YU6AwMDAwMTAwMDg10gqqTpGQaEpEMQihyTohwjgndnMYqk9L9FSDqZUE2dw3AbIaknjirJz7AbIenFiSInrsjwFCGpznAVWbJH/NZnCIuFgYGBgeE0XIbPI8aNofkDsqQQAwODPpOzDFs00/eTP1nOQlUyMjAwTEv/8IiBQY/xz7drJ88cfPlEkI0BoTProRUDA8OjjddOMDAwMKSJ3mPACVb+64QxmbBIb8AnyYBHklEVj+R/JjySDJb4jMVj5/b/OB1IJQAAg3ksR3QPgSAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2305CDC81C8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAAAAAA6I3INAAAAMElEQVR4nGNgoCmwC4SxmBgYGBgcglC4cQwoXCYUrp44CteLE4WrznAVRS/DaRgXANfSA++Pz/S5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=14x14 at 0x2305CEA4908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAAAAAA6I3INAAAARklEQVR4nGNgoDZwfqHOwMDAwMAE4ZqeYUDiMinKMSKplf63CFl2DsNtZC4/w24kteLP/8kicRf/uyEAY/OFbfrxLxrKAQAADA6jBUtHLwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=14x14 at 0x2305CEA4908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAAAAAA6I3INAAAAOElEQVR4nGNkYGCYHx++moFBn8lZho2FgYHh+/8ZVQwMeox/vl1jZGBgYCi3YmBgYNh47QTDkAAAazcLnD+w9doAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=14x14 at 0x2305CEA4908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAAAAAA6I3INAAAARElEQVR4nGP4+PnV5o9/NBigoD9ahvXlLwYkkPbvDozJxMDA4MywFpnLwLABlcuAwmVUReH+Z0JVbImqGEXv9v8MZAEAIZQPjGdyyzYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=14x14 at 0x2305CEA4908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist = torchvision.datasets.MNIST(\"dataset\", download=True)\n",
    "image = mnist.data[2]\n",
    "\n",
    "display(pil(image))\n",
    "#该函数的作用就是把Tensor数据变为完整原始的图片数据（保存后可以直接双击打开的那种）\n",
    "display(image.shape)\n",
    "\n",
    "result = einops.rearrange(\n",
    "    image.reshape((28, 28)), \"(pw w) (ph h) -> (pw ph) w h\", pw=2, ph=2\n",
    ")\n",
    "#重新指定维度，使得一张图片切成四块\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    display(pil(result[i]))\n",
    "\n",
    "# in the following code we use train_set or test_set instead of mnist.\n",
    "del mnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok let's try other things with einops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 49])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "a = torch.rand((1, 28, 28))\n",
    "\n",
    "rearrange(a, \"b (pw w) (ph h) -> b (pw ph) (w h)\", pw=4, ph=4).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got a problem: torch.cat doesn't work the same way as tensor addition. so we need to manually repeat the tensor in other channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [2., 2.],\n",
      "        [3., 3.]])\n",
      "tensor([[[1., 1.],\n",
      "         [2., 2.],\n",
      "         [3., 3.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [2., 2.],\n",
      "         [3., 3.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [2., 2.],\n",
      "         [3., 3.]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor(((1, 1), (2, 2), (3, 3)))\n",
    "print(a)\n",
    "a = a.reshape((1, 3, 2))\n",
    "print(einops.repeat(a, \"() h w -> repeat h w\", repeat=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks good to me\n",
    "\n",
    "now, let's try to create a vision transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but before that, let's try to figure out how pytorch handles tensor addition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([1., 2.])\n",
      "tensor([[2., 4.],\n",
      "        [4., 6.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor(((1, 2), (3, 4), (5, 6)))\n",
    "print(a)\n",
    "b = torch.Tensor((1, 2))\n",
    "print(b)\n",
    "\n",
    "print(a + b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok so it worked as i expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "a = (1, 2, 3)\n",
    "\n",
    "b, c, d = a\n",
    "\n",
    "print(b, c, d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, we need to load mnist data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "#torchvision.transforms是常见的图像预处理方法\n",
    "\n",
    "#transforms.Compose串联多个图片变换的操作\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]   #第一步，转为tenser类；第二个操作，归一化\n",
    ")\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    \"dataset\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_set = torchvision.datasets.MNIST(\"dataset\", train=False, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then, we define the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # shape: (batch, 28, 28) == (batch, 4*7, 4*7) -> (batch, 4*4, 7*7)\n",
    "        \n",
    "            #nn.Sequential 是一个时序容器，\n",
    "        self.patch_embedding = nn.Sequential(\n",
    "            Rearrange(\"b (pw w) (ph h) -> b (pw ph) (w h)\", pw=4, ph=4),\n",
    "            nn.Linear(7 * 7, 7 * 7),\n",
    "        )\n",
    "        \n",
    "        #有重复part的分块\n",
    "        \n",
    "        # shape: (batch, 4*4+1, 7*7)\n",
    "        self.class_token = nn.parameter.Parameter(torch.randn((1, 1, 7 * 7)))\n",
    "        # shape: same\n",
    "        self.position_encodings = nn.parameter.Parameter(\n",
    "            torch.randn((1, 4 * 4 + 1, 7 * 7))\n",
    "        )\n",
    "        # the actual transformer, we use the transformer provided by pytorch here.\n",
    "        # d_model is the dimention for the embedding vector\n",
    "        # n_head is the number of heads in the multi head self attention thing\n",
    "        # d_model has to be divisible by n_head\n",
    "        # dim_feedforward is the number of hidden nodes in hidden layer in feedforward block\n",
    "        # batch_first means the first dim of the tensor is batch\n",
    "        # shape: same\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=7 * 7, nhead=7, dim_feedforward=128, batch_first=True\n",
    "            ),\n",
    "            3,\n",
    "        )\n",
    "        # the final MLP\n",
    "        # shape: (batch, 4*4+1, 7*7) -> (batch, 10)\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(7 * 7), nn.Linear(7 * 7, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        batch_size, _, _ = x.shape\n",
    "        class_token = einops.repeat(\n",
    "            self.class_token,\n",
    "            \"() words features -> repeat words features\",\n",
    "            repeat=batch_size,\n",
    "        )\n",
    "        x = torch.cat((class_token, x), dim=-2)\n",
    "        x += self.position_encodings\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x[:, 0]\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try to use this absolute abomination we've created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4396, -0.6882, -0.5664,  0.1234, -0.4082, -0.2944,  0.5940, -0.0736,\n",
       "         -0.0883, -0.5515]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_transformer_model = VisionTransformer()\n",
    "vision_transformer_model(torch.rand((1, 28, 28)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, it works. And the shape seems to be alright.\n",
    "\n",
    "now let's try to train it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, i: 0/60000, loss: 2.3846499919891357\n",
      "epoch: 0, i: 2560/60000, loss: 2.137324094772339\n",
      "epoch: 0, i: 5120/60000, loss: 1.6387113332748413\n",
      "epoch: 0, i: 7680/60000, loss: 1.3975093364715576\n",
      "epoch: 0, i: 10240/60000, loss: 0.9649822115898132\n",
      "epoch: 0, i: 12800/60000, loss: 0.9680061340332031\n",
      "epoch: 0, i: 15360/60000, loss: 0.7219030857086182\n",
      "epoch: 0, i: 17920/60000, loss: 0.5770267248153687\n",
      "epoch: 0, i: 20480/60000, loss: 0.5623785257339478\n",
      "epoch: 0, i: 23040/60000, loss: 0.5560121536254883\n",
      "epoch: 0, i: 25600/60000, loss: 0.44870585203170776\n",
      "epoch: 0, i: 28160/60000, loss: 0.5034988522529602\n",
      "epoch: 0, i: 30720/60000, loss: 0.47477445006370544\n",
      "epoch: 0, i: 33280/60000, loss: 0.4478832483291626\n",
      "epoch: 0, i: 35840/60000, loss: 0.35157737135887146\n",
      "epoch: 0, i: 38400/60000, loss: 0.3507702648639679\n",
      "epoch: 0, i: 40960/60000, loss: 0.3347446322441101\n",
      "epoch: 0, i: 43520/60000, loss: 0.2557900846004486\n",
      "epoch: 0, i: 46080/60000, loss: 0.41349756717681885\n",
      "epoch: 0, i: 48640/60000, loss: 0.23282039165496826\n",
      "epoch: 0, i: 51200/60000, loss: 0.2568291425704956\n",
      "epoch: 0, i: 53760/60000, loss: 0.30595675110816956\n",
      "epoch: 0, i: 56320/60000, loss: 0.22415496408939362\n",
      "epoch: 0, i: 58880/60000, loss: 0.08016452938318253\n",
      "epoch: 1, i: 0/60000, loss: 0.23935522139072418\n",
      "epoch: 1, i: 2560/60000, loss: 0.21414326131343842\n",
      "epoch: 1, i: 5120/60000, loss: 0.2849753499031067\n",
      "epoch: 1, i: 7680/60000, loss: 0.24972771108150482\n",
      "epoch: 1, i: 10240/60000, loss: 0.20330274105072021\n",
      "epoch: 1, i: 12800/60000, loss: 0.28444820642471313\n",
      "epoch: 1, i: 15360/60000, loss: 0.2292613983154297\n",
      "epoch: 1, i: 17920/60000, loss: 0.19070522487163544\n",
      "epoch: 1, i: 20480/60000, loss: 0.23974080383777618\n",
      "epoch: 1, i: 23040/60000, loss: 0.20701557397842407\n",
      "epoch: 1, i: 25600/60000, loss: 0.18873222172260284\n",
      "epoch: 1, i: 28160/60000, loss: 0.2410302460193634\n",
      "epoch: 1, i: 30720/60000, loss: 0.2750316858291626\n",
      "epoch: 1, i: 33280/60000, loss: 0.26227739453315735\n",
      "epoch: 1, i: 35840/60000, loss: 0.2510726749897003\n",
      "epoch: 1, i: 38400/60000, loss: 0.19745704531669617\n",
      "epoch: 1, i: 40960/60000, loss: 0.19579650461673737\n",
      "epoch: 1, i: 43520/60000, loss: 0.14302101731300354\n",
      "epoch: 1, i: 46080/60000, loss: 0.29481059312820435\n",
      "epoch: 1, i: 48640/60000, loss: 0.15815763175487518\n",
      "epoch: 1, i: 51200/60000, loss: 0.1701674908399582\n",
      "epoch: 1, i: 53760/60000, loss: 0.20922112464904785\n",
      "epoch: 1, i: 56320/60000, loss: 0.1453162431716919\n",
      "epoch: 1, i: 58880/60000, loss: 0.03416159376502037\n",
      "epoch: 2, i: 0/60000, loss: 0.2025260627269745\n",
      "epoch: 2, i: 2560/60000, loss: 0.18744507431983948\n",
      "epoch: 2, i: 5120/60000, loss: 0.20488671958446503\n",
      "epoch: 2, i: 7680/60000, loss: 0.18011657893657684\n",
      "epoch: 2, i: 10240/60000, loss: 0.15346507728099823\n",
      "epoch: 2, i: 12800/60000, loss: 0.230249285697937\n",
      "epoch: 2, i: 15360/60000, loss: 0.17831122875213623\n",
      "epoch: 2, i: 17920/60000, loss: 0.13472236692905426\n",
      "epoch: 2, i: 20480/60000, loss: 0.19907765090465546\n",
      "epoch: 2, i: 23040/60000, loss: 0.14441099762916565\n",
      "epoch: 2, i: 25600/60000, loss: 0.1668912023305893\n",
      "epoch: 2, i: 28160/60000, loss: 0.21607348322868347\n",
      "epoch: 2, i: 30720/60000, loss: 0.2261652797460556\n",
      "epoch: 2, i: 33280/60000, loss: 0.21843300759792328\n",
      "epoch: 2, i: 35840/60000, loss: 0.2036600261926651\n",
      "epoch: 2, i: 38400/60000, loss: 0.1406722068786621\n",
      "epoch: 2, i: 40960/60000, loss: 0.14997872710227966\n",
      "epoch: 2, i: 43520/60000, loss: 0.1369611918926239\n",
      "epoch: 2, i: 46080/60000, loss: 0.2540164291858673\n",
      "epoch: 2, i: 48640/60000, loss: 0.12030845135450363\n",
      "epoch: 2, i: 51200/60000, loss: 0.14657534658908844\n",
      "epoch: 2, i: 53760/60000, loss: 0.18223561346530914\n",
      "epoch: 2, i: 56320/60000, loss: 0.11914853751659393\n",
      "epoch: 2, i: 58880/60000, loss: 0.03889654204249382\n",
      "epoch: 3, i: 0/60000, loss: 0.18719474971294403\n",
      "epoch: 3, i: 2560/60000, loss: 0.16878235340118408\n",
      "epoch: 3, i: 5120/60000, loss: 0.1947423219680786\n",
      "epoch: 3, i: 7680/60000, loss: 0.1559428572654724\n",
      "epoch: 3, i: 10240/60000, loss: 0.12802349030971527\n",
      "epoch: 3, i: 12800/60000, loss: 0.19547973573207855\n",
      "epoch: 3, i: 15360/60000, loss: 0.17574863135814667\n",
      "epoch: 3, i: 17920/60000, loss: 0.12703850865364075\n",
      "epoch: 3, i: 20480/60000, loss: 0.17012816667556763\n",
      "epoch: 3, i: 23040/60000, loss: 0.10935904830694199\n",
      "epoch: 3, i: 25600/60000, loss: 0.15159156918525696\n",
      "epoch: 3, i: 28160/60000, loss: 0.16562126576900482\n",
      "epoch: 3, i: 30720/60000, loss: 0.20371145009994507\n",
      "epoch: 3, i: 33280/60000, loss: 0.18549472093582153\n",
      "epoch: 3, i: 35840/60000, loss: 0.18548189103603363\n",
      "epoch: 3, i: 38400/60000, loss: 0.1463484764099121\n",
      "epoch: 3, i: 40960/60000, loss: 0.13810677826404572\n",
      "epoch: 3, i: 43520/60000, loss: 0.11768291145563126\n",
      "epoch: 3, i: 46080/60000, loss: 0.18698185682296753\n",
      "epoch: 3, i: 48640/60000, loss: 0.092319056391716\n",
      "epoch: 3, i: 51200/60000, loss: 0.13901692628860474\n",
      "epoch: 3, i: 53760/60000, loss: 0.1751347929239273\n",
      "epoch: 3, i: 56320/60000, loss: 0.13489489257335663\n",
      "epoch: 3, i: 58880/60000, loss: 0.02920967899262905\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 4\n",
    "batch_size = 256\n",
    "optimizer = torch.optim.Adam(vision_transformer_model.parameters(), lr=0.001)\n",
    "# for every step, this scheduler times the lr by gamma\n",
    "# step_size if how many steps before it times the lr by gamma\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "\n",
    "vision_transformer_model.to(device)\n",
    "vision_transformer_model.train()\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x = einops.rearrange(x, \"b c h w -> (b c) h w\")\n",
    "        optimizer.zero_grad()\n",
    "        output = vision_transformer_model(x.to(device))\n",
    "        loss = criterion(output, y.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                f\"epoch: {epoch}, i: {batch_idx*len(x)}/{len(train_loader.dataset)}, loss: {loss.item()}\"\n",
    "            )\n",
    "\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like a great success to me\n",
    "\n",
    "( actually, as i was creating this notebook, many things went wrong. But thanks to the magic of _editing_, everything seems to be fine :) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0/10000\n",
      "i: 2560/10000\n",
      "i: 5120/10000\n",
      "i: 7680/10000\n",
      "correct: 9637/10000, 96.37%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "vision_transformer_model.to(device)\n",
    "vision_transformer_model.eval()\n",
    "correct_count = 0\n",
    "for batch_idx, (x, y) in enumerate(test_loader):\n",
    "    x = einops.rearrange(x, \"b c h w -> (b c) h w\")\n",
    "    output = vision_transformer_model(x.to(device))\n",
    "    correct_count += output.argmax(dim=-1).eq(y.to(device)).count_nonzero()\n",
    "\n",
    "    if batch_idx % 10 == 0:\n",
    "        print(f\"i: {batch_idx*len(x)}/{len(test_loader.dataset)}\")\n",
    "\n",
    "print(\n",
    "    f\"correct: {correct_count}/{len(test_loader.dataset)}, {correct_count/len(test_loader.dataset)*100:.2f}%\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's compare the model with a regular CNN model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0458,  0.1063, -0.0766,  0.0239, -0.0795, -0.1176,  0.0239,  0.0858,\n",
       "         -0.0522,  0.1815]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model = nn.Sequential(\n",
    "    nn.Conv2d(\n",
    "        in_channels=1,\n",
    "        out_channels=16,\n",
    "        kernel_size=5,\n",
    "        stride=1,\n",
    "        padding=\"same\",\n",
    "    ),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Conv2d(16, 32, 5, 1, \"same\"),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    # defaults: start_dim: 1, end_dim: -1\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32 * 7 * 7, 10),\n",
    ")\n",
    "cnn_model(torch.randn((1, 1, 28, 28)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, i: 0/60000, loss: 2.302013397216797\n",
      "epoch: 0, i: 2560/60000, loss: 1.0947916507720947\n",
      "epoch: 0, i: 5120/60000, loss: 0.5394686460494995\n",
      "epoch: 0, i: 7680/60000, loss: 0.49899035692214966\n",
      "epoch: 0, i: 10240/60000, loss: 0.3036370873451233\n",
      "epoch: 0, i: 12800/60000, loss: 0.39212530851364136\n",
      "epoch: 0, i: 15360/60000, loss: 0.23711681365966797\n",
      "epoch: 0, i: 17920/60000, loss: 0.17943309247493744\n",
      "epoch: 0, i: 20480/60000, loss: 0.26187723875045776\n",
      "epoch: 0, i: 23040/60000, loss: 0.16979683935642242\n",
      "epoch: 0, i: 25600/60000, loss: 0.18883703649044037\n",
      "epoch: 0, i: 28160/60000, loss: 0.20958547294139862\n",
      "epoch: 0, i: 30720/60000, loss: 0.2108856588602066\n",
      "epoch: 0, i: 33280/60000, loss: 0.1452208012342453\n",
      "epoch: 0, i: 35840/60000, loss: 0.16448284685611725\n",
      "epoch: 0, i: 38400/60000, loss: 0.12642329931259155\n",
      "epoch: 0, i: 40960/60000, loss: 0.13525404036045074\n",
      "epoch: 0, i: 43520/60000, loss: 0.1044606938958168\n",
      "epoch: 0, i: 46080/60000, loss: 0.21315431594848633\n",
      "epoch: 0, i: 48640/60000, loss: 0.06428398191928864\n",
      "epoch: 0, i: 51200/60000, loss: 0.10275214910507202\n",
      "epoch: 0, i: 53760/60000, loss: 0.12862031161785126\n",
      "epoch: 0, i: 56320/60000, loss: 0.09653998166322708\n",
      "epoch: 0, i: 58880/60000, loss: 0.015017836354672909\n",
      "epoch: 1, i: 0/60000, loss: 0.1261346936225891\n",
      "epoch: 1, i: 2560/60000, loss: 0.12479181587696075\n",
      "epoch: 1, i: 5120/60000, loss: 0.11744926124811172\n",
      "epoch: 1, i: 7680/60000, loss: 0.08304887264966965\n",
      "epoch: 1, i: 10240/60000, loss: 0.10141521692276001\n",
      "epoch: 1, i: 12800/60000, loss: 0.09482850879430771\n",
      "epoch: 1, i: 15360/60000, loss: 0.07957613468170166\n",
      "epoch: 1, i: 17920/60000, loss: 0.07519614696502686\n",
      "epoch: 1, i: 20480/60000, loss: 0.11710209399461746\n",
      "epoch: 1, i: 23040/60000, loss: 0.04927976801991463\n",
      "epoch: 1, i: 25600/60000, loss: 0.09740113466978073\n",
      "epoch: 1, i: 28160/60000, loss: 0.10109048336744308\n",
      "epoch: 1, i: 30720/60000, loss: 0.08704481273889542\n",
      "epoch: 1, i: 33280/60000, loss: 0.07777773588895798\n",
      "epoch: 1, i: 35840/60000, loss: 0.0810946524143219\n",
      "epoch: 1, i: 38400/60000, loss: 0.07101497054100037\n",
      "epoch: 1, i: 40960/60000, loss: 0.07933562994003296\n",
      "epoch: 1, i: 43520/60000, loss: 0.06999730318784714\n",
      "epoch: 1, i: 46080/60000, loss: 0.13745254278182983\n",
      "epoch: 1, i: 48640/60000, loss: 0.03450461104512215\n",
      "epoch: 1, i: 51200/60000, loss: 0.07948469370603561\n",
      "epoch: 1, i: 53760/60000, loss: 0.0672876164317131\n",
      "epoch: 1, i: 56320/60000, loss: 0.05818997696042061\n",
      "epoch: 1, i: 58880/60000, loss: 0.005465431604534388\n",
      "epoch: 2, i: 0/60000, loss: 0.09102270007133484\n",
      "epoch: 2, i: 2560/60000, loss: 0.10295604914426804\n",
      "epoch: 2, i: 5120/60000, loss: 0.07848423719406128\n",
      "epoch: 2, i: 7680/60000, loss: 0.05287085101008415\n",
      "epoch: 2, i: 10240/60000, loss: 0.0809500440955162\n",
      "epoch: 2, i: 12800/60000, loss: 0.05106237158179283\n",
      "epoch: 2, i: 15360/60000, loss: 0.06202169135212898\n",
      "epoch: 2, i: 17920/60000, loss: 0.05166665464639664\n",
      "epoch: 2, i: 20480/60000, loss: 0.092116579413414\n",
      "epoch: 2, i: 23040/60000, loss: 0.0359836108982563\n",
      "epoch: 2, i: 25600/60000, loss: 0.07959507405757904\n",
      "epoch: 2, i: 28160/60000, loss: 0.08030559122562408\n",
      "epoch: 2, i: 30720/60000, loss: 0.06910333037376404\n",
      "epoch: 2, i: 33280/60000, loss: 0.06630463898181915\n",
      "epoch: 2, i: 35840/60000, loss: 0.056156568229198456\n",
      "epoch: 2, i: 38400/60000, loss: 0.05839303508400917\n",
      "epoch: 2, i: 40960/60000, loss: 0.06241757422685623\n",
      "epoch: 2, i: 43520/60000, loss: 0.05519001930952072\n",
      "epoch: 2, i: 46080/60000, loss: 0.1135716512799263\n",
      "epoch: 2, i: 48640/60000, loss: 0.025889212265610695\n",
      "epoch: 2, i: 51200/60000, loss: 0.06594574451446533\n",
      "epoch: 2, i: 53760/60000, loss: 0.05086852237582207\n",
      "epoch: 2, i: 56320/60000, loss: 0.05383870005607605\n",
      "epoch: 2, i: 58880/60000, loss: 0.003882430261000991\n",
      "epoch: 3, i: 0/60000, loss: 0.08027557283639908\n",
      "epoch: 3, i: 2560/60000, loss: 0.0940665528178215\n",
      "epoch: 3, i: 5120/60000, loss: 0.06433775275945663\n",
      "epoch: 3, i: 7680/60000, loss: 0.045365430414676666\n",
      "epoch: 3, i: 10240/60000, loss: 0.07613257318735123\n",
      "epoch: 3, i: 12800/60000, loss: 0.04218965396285057\n",
      "epoch: 3, i: 15360/60000, loss: 0.049149248749017715\n",
      "epoch: 3, i: 17920/60000, loss: 0.04556319862604141\n",
      "epoch: 3, i: 20480/60000, loss: 0.08160168677568436\n",
      "epoch: 3, i: 23040/60000, loss: 0.0289972722530365\n",
      "epoch: 3, i: 25600/60000, loss: 0.07052958756685257\n",
      "epoch: 3, i: 28160/60000, loss: 0.06512736529111862\n",
      "epoch: 3, i: 30720/60000, loss: 0.05759443715214729\n",
      "epoch: 3, i: 33280/60000, loss: 0.05919553339481354\n",
      "epoch: 3, i: 35840/60000, loss: 0.04790477082133293\n",
      "epoch: 3, i: 38400/60000, loss: 0.0537705197930336\n",
      "epoch: 3, i: 40960/60000, loss: 0.05696076527237892\n",
      "epoch: 3, i: 43520/60000, loss: 0.04959951713681221\n",
      "epoch: 3, i: 46080/60000, loss: 0.09684083610773087\n",
      "epoch: 3, i: 48640/60000, loss: 0.02504144050180912\n",
      "epoch: 3, i: 51200/60000, loss: 0.06078748777508736\n",
      "epoch: 3, i: 53760/60000, loss: 0.04793667048215866\n",
      "epoch: 3, i: 56320/60000, loss: 0.04772936552762985\n",
      "epoch: 3, i: 58880/60000, loss: 0.004032688681036234\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 4\n",
    "batch_size = 256\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "# for every step, this scheduler times the lr by gamma\n",
    "# step_size if how many steps before it times the lr by gamma\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "\n",
    "cnn_model.to(device)\n",
    "cnn_model.train()\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = cnn_model(x.to(device))\n",
    "        loss = criterion(output, y.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                f\"epoch: {epoch}, i: {batch_idx*len(x)}/{len(train_loader.dataset)}, loss: {loss.item()}\"\n",
    "            )\n",
    "\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([[1, 2, 3], [1, 2, 1], [1, 2, 1]])\n",
    "b = torch.Tensor([2, 1, 0])\n",
    "a.argmax(dim=-1).eq(b).count_nonzero()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0/10000\n",
      "i: 2560/10000\n",
      "i: 5120/10000\n",
      "i: 7680/10000\n",
      "correct: 9844/10000, 98.44%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "cnn_model.to(device)\n",
    "cnn_model.eval()\n",
    "correct_count = 0\n",
    "for batch_idx, (x, y) in enumerate(test_loader):\n",
    "    output = cnn_model(x.to(device))\n",
    "    correct_count += output.argmax(dim=-1).eq(y.to(device)).count_nonzero()\n",
    "\n",
    "    if batch_idx % 10 == 0:\n",
    "        print(f\"i: {batch_idx*len(x)}/{len(test_loader.dataset)}\")\n",
    "\n",
    "print(\n",
    "    f\"correct: {correct_count}/{len(test_loader.dataset)}, {correct_count/len(test_loader.dataset)*100:.2f}%\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a7d19907b29b73a76e90ccf1e029f1f2d2743eb60f7b8c1e389ad5828c35a33"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
