{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ptroch 自动微分\n",
    "* autograd 软件包为Tensors上的所有操作提供自动微分\n",
    "* 它是一个由运行定义的框架\n",
    " * 意味着以代码运行方式定义你的后向传播，并且每次迭代都可以不同\n",
    "#### TENSOR\n",
    "* torch.Tensor 是包的核心类。\n",
    "  * 如果将其属性 .requires_grad 设置为 True，则会开始跟踪针对 tensor 的所有操作。\n",
    "  * 完成计算后，您可以调用 .backward() 来自动计算所有梯度。该张量的梯度将累积到 .grad 属性中。\n",
    "* 要停止 tensor 历史记录的跟踪，您可以调用 .detach()，它将其与计算历史记录分离，并防止将来的计算被跟踪。\n",
    "* 停止跟踪历史记录（和使用内存），您还可以将代码块使用 with torch.no_grad(): 包装起来。在评估模型时，这是特别有用，因为模型在训练阶段具有 requires_grad = True 的可训练参数有利于调参，但在评估阶段我们不需要梯度。\n",
    "* Function\n",
    "  * Tensor 和 Function 互相连接并构建一个非循环图，它保存整个完整的计算过程的历史信息\n",
    "  * 每个张量都有一个 .grad_fn 属性保存着创建了张量的 Function 的引用，（如果用户自己创建张量，则g rad_fn 是 None ）\n",
    "* Tensor.backward()\n",
    "  * 计算导数\n",
    "  * Tensor 是标量（即它包含一个元素数据），则不需要指定任何参数backward()，但是如果它有更多元素，则需要指定一个gradient 参数来指定张量的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x000002020D3A3D60>\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 创建一个张量，设置require_grad = True 来跟踪与它相关的计算\n",
    "x = torch.ones(2,2,requires_grad = True)\n",
    "print(x)\n",
    "\n",
    "y=x+2\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "\n",
    "z = y*y*3\n",
    "out=z.mean()\n",
    "print(z,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x00000202145C9F10>\n"
     ]
    }
   ],
   "source": [
    "# .requires_grad_( ... ) 会改变张量的 requires_grad 标记。输入的标记默认为 False ，如果没有提供相应的参数\n",
    "a=torch.randn(2,2)\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b=(a*a).sum()\n",
    "print(b.grad_fn)\n",
    "\n",
    "del a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "# 梯度\n",
    "# 后向传播\n",
    "out.backward()\n",
    "print(x.grad)\n",
    "\n",
    "del x,y,z,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3887,  1.2233,  1.3086], requires_grad=True)\n",
      "tensor([-2.7775,  2.4465,  2.6171], grad_fn=<MulBackward0>)\n",
      "tensor([-711.0353,  626.3071,  669.9870], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 雅可比向量积\n",
    "x=torch.randn(3,requires_grad=True)\n",
    "print(x)\n",
    "y=x*2\n",
    "print(y)\n",
    "while y.data.norm() <1000:\n",
    "    y=y*2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be203ce0b3afc4f5c37fbac412025d7ed1d67cabe9dd00b1fc8774c6d6d19d70"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
