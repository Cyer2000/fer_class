{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络\n",
    "* 通过 torch.nn 包来构建\n",
    "* 神经网络是基于自动梯度 (autograd)来定义一些模型\n",
    "* 一个 nn.Module 包括层和一个方法 \n",
    "  * forward(input) 它会返回输出(output)\n",
    "#### 典型的神经网络训练过程\n",
    "* 定义一个包含可训练参数的神经网络\n",
    "* 迭代整个输入\n",
    "* 通过神经网络了处理输入\n",
    "* 计算损失\n",
    "* 反向传播梯度到神经网络的参数\n",
    "* 更新网络的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义神经网络\n",
    "##### nn.Conv1d\n",
    "* 用于文本数据，只对宽度进行卷积，对高度不卷积\n",
    "  * 通常，输入大小为word_embedding_dim * max_length，其中，word_embedding_dim为词向量的维度，max_length为句子的最大长度\n",
    "  * 卷积核窗口在句子长度的方向上滑动，进行卷积操作\n",
    "* 主要参数说明\n",
    "  * in_channels：在文本应用中，即为词向量的维度\n",
    "  * out_channels：卷积产生的通道数，有多少个out_channels，就需要多少个一维卷积（也就是卷积核的数量）\n",
    "  * kernel_size：卷积核的尺寸；卷积核的第二个维度由in_channels决定，所以实际上卷积核的大小为kernel_size * in_channels\n",
    "  * padding：对输入的每一条边，补充0的层数\n",
    "\n",
    "##### nn.Conv2d\n",
    "* 对由多个输入平面组成的输入信号进行二维卷积\n",
    "* 主要参数说明：\n",
    "  * in_channels —— 输入的channels数\n",
    "  * out_channels —— 输出的channels数\n",
    "  * kernel_size ——卷积核的尺寸，可以是方形卷积核、也可以不是\n",
    "  * stride —— 步长，用来控制卷积核移动间隔\n",
    "  * padding ——输入边沿扩边操作\n",
    "  * padding_mode ——扩边的方式\n",
    "  * bias ——是否使用偏置(即out = wx+b中的b)\n",
    "  * dilation —— 这个参数简单说，设定了取数之间的间隔\n",
    "  * groups —— 进行分组卷积的组数\n",
    "    * controls the connections between inputs and poutputs\n",
    "    * in_channels and out_channels must both divisible by groups\n",
    "##### MaxPool2d\n",
    "* 主要参数\n",
    "  * kernel_size ：表示做最大池化的窗口大小，可以是单个值，也可以是tuple元组\n",
    "    * 滑动窗口，非卷积核，大小由自己制定\n",
    "    * 输入单值3 即3×3，输入元组（3，2）即3×2\n",
    "    * 取该窗口覆盖元素中的最大值\n",
    "  * stride ：步长，可以是单个值，也可以是tuple元组\n",
    "    * 确定窗口如何滑动\n",
    "    * 默认为池化窗口大小\n",
    "  * padding ：填充，可以是单个值，也可以是tuple元组\n",
    "  * ilation ：控制窗口中元素步幅\n",
    "  * return_indices ：布尔类型，返回最大值位置索引\n",
    "  * ceil_mode ：布尔类型，为True，用向上取整的方法，计算输出形状；默认是向下取整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()  #继承父类的init方法\n",
    "        # input: 1 channel 图像； output：6 channels ；5 ×5 卷积核\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation（仿射函数）反映了一种从 k 维到 m 维的空间映射关系\n",
    "        # y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # in_feature,out_feature,bias\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # max pooling over a (2,2) window\n",
    "        print(\"输入形状：\",x.size())\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        print(\"第一层卷积网络输出形状：\", x.size())\n",
    "        # if the side is a suare, you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        print(\"第二层卷积网络输出形状1：\", x.size())\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        print(\"第二层卷积网络输出形状2：\", x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        print(\"全连接层1输出形状：\", x.size())\n",
    "        x = F.relu(self.fc2(x))\n",
    "        print(\"全连接层2输出形状：\", x.size())\n",
    "        x = self.fc3(x)\n",
    "        print(\"全连接层3输出形状：\", x.size())\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理输入，调用反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "输入形状： torch.Size([1, 1, 32, 32])\n",
      "第一层卷积网络输出形状： torch.Size([1, 6, 14, 14])\n",
      "第二层卷积网络输出形状1： torch.Size([1, 16, 5, 5])\n",
      "第二层卷积网络输出形状2： torch.Size([1, 400])\n",
      "全连接层1输出形状： torch.Size([1, 120])\n",
      "全连接层2输出形状： torch.Size([1, 84])\n",
      "全连接层3输出形状： torch.Size([1, 10])\n",
      "tensor([[ 0.1051, -0.0245,  0.0836, -0.0030,  0.0028,  0.0225, -0.1032, -0.0201,\n",
      "          0.0256, -0.0133]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "#返回模型的可训练参数 net.parameters()\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())\n",
    "\n",
    "# 尝试随机生成一个32×32的输入\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "output = net(input)\n",
    "print(output)\n",
    "\n",
    "# 将所有参数梯度缓存器置零，用随机的梯度来反向传播\n",
    "net.zero_grad()\n",
    "output.backward(torch.randn(1, 10)) # 随机梯度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算损失值\n",
    "* 需要模型输出和目标，然后计算一个值来评估输出距离目标由多远\n",
    "* 不同的损失函数在nn包中\n",
    "    * 简单的损失函数就是nn.MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状： torch.Size([1, 1, 32, 32])\n",
      "第一层卷积网络输出形状： torch.Size([1, 6, 14, 14])\n",
      "第二层卷积网络输出形状1： torch.Size([1, 16, 5, 5])\n",
      "第二层卷积网络输出形状2： torch.Size([1, 400])\n",
      "全连接层1输出形状： torch.Size([1, 120])\n",
      "全连接层2输出形状： torch.Size([1, 84])\n",
      "全连接层3输出形状： torch.Size([1, 10])\n",
      "目标值的形状 torch.Size([1, 10])\n",
      "tensor(0.4571, grad_fn=<MseLossBackward0>)\n",
      "<MseLossBackward0 object at 0x000001FA3BA028B0>\n",
      "<AddmmBackward0 object at 0x000001FA3BA029D0>\n",
      "<AccumulateGrad object at 0x000001FA3BA028B0>\n",
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0011, -0.0055, -0.0038,  0.0003,  0.0041,  0.0002])\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "\n",
    "target = torch.randn(10)  #假设值\n",
    "target = target.view(1, -1)  #make it the same shape as output\n",
    "print('目标值的形状', target.size())\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "\n",
    "# 跟随以下步骤来反向传播\n",
    "print(loss.grad_fn)\n",
    "print(loss.grad_fn.next_functions[0][0])\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])\n",
    "\n",
    "# 清空现存的梯度\n",
    "# 调用loss.backward()\n",
    "# 看conv1的偏置项在反向传播前后的变化\n",
    "net.zero_grad()\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新神经网络的参数\n",
    "* 最简单的更新规则：随机梯度下降\n",
    "* weight=weight-learning_rate*gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate= 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be203ce0b3afc4f5c37fbac412025d7ed1d67cabe9dd00b1fc8774c6d6d19d70"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
