[toc]



### 1、Vit**   (vision Transformer)

* **原版Vit**
  * 可以通过SAM使得模型在没有预训练的基础上提高泛化性能
* **改进原因**：
  * transformer只有在大型数据集上进行预训练才能发挥优势，且泛化能力低，因此考虑如何能使transformer在不进行预训练时达到与resnet等相似的性能。
* **改进方法**：
  * 改进优化器，利用SAM提高模型的泛化性能。由于一阶优化器SGD、Adam忽略了与泛化相关的高阶信息如平滑度等，而通过SAM优化器能够帮助模型表现出更加平滑的损失，极大提高模型的泛化能力。
  * 数据增强和正则化
* **模型图**![vit_figure](E:\desktop\WY\MY\18\唐舒琪\模型图\vit_figure.png)
* **参考链接：**
  *  [Vision Transformer 超详细解读 (原理分析+代码解读) (一) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/340149804) 
  *  [(26条消息) Vision Transformer详解_霹雳吧啦Wz-CSDN博客_wz框架](https://blog.csdn.net/qq_37541097/article/details/118242600?spm=1001.2101.3001.6650.1&depth_1-utm_relevant_index=2) 
* **代码链接**
  * https://github.com/google-research/vision_transformer





### 2、CVT (Convolutional vision Transformer)

* **改进类型**：

  * 引入CNN来提高精度与效率的同时补偿因分辨率下降带来的损失
  
* **改进原因：** 

  * ViT的性能弱于具有相似的尺寸的CNN的性能，而且，ViT所需要的训练数据量要比同尺寸的CNN模型大很多 。
  * CNN 考虑的是一张图片的空间上相邻的那些信息 ，它们是高度相关的，而transformer会考虑图片的全局信息。因此，CVT想要transformer获得与CNN相似的特性，从而减小数据集。

* **改进具体方法**：

  * 引入 Convolutional Token Embedding 对输入的 2D reshaped token map上做一次卷积操作，其目的是保证与CNN效果类似，都是随着层数的加深而逐渐减少token的数量（feature resolution）和逐渐增加token width（feature dimension）。 
  * 引入 Convolutional Projection （ Depth-wise separable convolution ）代替 Transformer的Block中的Linear Projection 操作， 这种卷积操作可以补偿分辨率下降带来的损失 。
  * 其它变化
    * 不再使用位置编码。
    * class token只加在最后一个stage里面。

* **模型图：**

  ![1641739798339](E:\desktop\WY\MY\18\唐舒琪\模型图\CvT.png)

* **参考链接**

  *  [Vision Transformer 超详细解读 (原理分析+代码解读) (六) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/361112935) 
  *   [CvT: Introducing Convolutions to Vision Transformers论文笔记 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/430720867) 
  *  [Transformer\]CvT:Introducing Convolutions to Vision Transformers_黄小米吖的博客-CSDN博客](https://blog.csdn.net/qq_37151108/article/details/122210654?ops_request_misc=%7B%22request%5Fid%22%3A%22164174337816780255250280%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164174337816780255250280&biz_id=0&spm=1018.2226.3001.4187) 

* **代码链接**
  
  * https://github.com/leoxiaobin/CvT





### 3、caiT  (Class-Attention in Image Transformers)   与deiT相关

* **改进类型**：

  * 使模型易于训练，提高精度的同时还能高效处理class token
  
* **改进原因：**

  *  由于网络架构和优化是相互影响、相互作用的 , 因此如何改进架构使其能够更好地归一化、训练权重和初始化参数是很重要的。
  * 对于Vision transformer来说，ViT在优化参数的时候，其本身需要做到的两件事“1 引导attention过程并得到attention map“和”2 将token输入到classifier中完成分类” 对于参数优化是有矛盾作用的，因此作者希望能够延迟引入class token。

* **改进具体方法**：

  * 利用LayerScale辅助优化，即保持Layer Normalization，并对Self-attention或者FFN的输出乘以一个对角矩阵。以这种方式训练网络能够使得vision transformer更加稳定，并且更加易于收敛、提高精度。
  * 引入 class-attention layers 高效处理class token。首先，caiT的self-attention和ViT是一致的，但是不再使用class token。 其次，在网络的最后两层，将self-attention变成class-attention层，这个层的作用是只将patch embedding的信息传递给class embedding以使class embedding获得整张图片的全部信息，但是不将class embedding的信息传递给patch embedding ，并且最后也只有class embedding会传递给FFN进行分类

* **模型图：**

  ![caiT](模型图/caiT.jpg)

* **参考链接**

  *   [Vision Transformer 超详细解读 (原理分析+代码解读) (八) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/363370678) 
  *   [(26条消息) 论文笔记【2】-- Cait : Going deeper with Image Transformers_ZONGYINLIU的博客-CSDN博客](https://blog.csdn.net/weixin_51391591/article/details/120614820?ops_request_misc=%7B%22request%5Fid%22%3A%22164174424516780274165723%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164174424516780274165723&biz_id=0&spm=1018.2226.3001.4187) 

* **代码链接**

  * https://github.com/facebookresearch/deit





### **4、NesT    (Nested Hierarchical Transformer)**   比较冷门

* **改进类型**：

  * 简化结构，收敛更快、精度更高，在小数据集也能实现良好的泛化性能

* **改进原因：**

  * 对于vision transformer，分层嵌套结构虽然被大量使用，但是其需要庞大的数据集。
  * 块聚合功能对于不同的块之间的信息通信起着重要作用

* **改进具体方法：**

  * 针对分层嵌套的transformers，将其与块聚合功能结合在一起能够使得其性能超过复杂的self-attention。

* **模型图：**

  <img src="模型图/NesT.png" alt="NesT" style="zoom:80%;" />

* **参考链接：**

  *  [谷歌《Aggregating Nested Transformers》：精度更好、数据效率更高、收敛性更快 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/375633171) 

* **代码链接：**

  * https://github.com/google-research/nested-transformer







### 5、PVT （Pyramid Vision Transformer）

* **改进类型**：

  * 减小self-attention的计算量和显存消耗并且使模型更加灵活

* **改进原因**

  * 原版的ViT在每个stage的feature map的输入输出都是相同的，且最后输出的feature map是单尺度的且分辨率低（因为它不接受细粒度的图像块作为输入）
  * 由于VIT中feature map一直是一样的（没有减小），其self-attention会带来高额的计算量和显存消耗

* **改进具体方法**

  * 在self-attention之前添加SRA层，减小feature map的尺寸，降低计算量。
  * 将多个transformer叠加在一起，随着网络的加深逐渐减小transformer输出的feature map 的大小，使得transformer能够接受不同大小的输入，并且以在不同transformer阶段生成不同比例，通道的特征图 
  * PVT接受细粒度的图像块（4X4）作为输入来学习到高分辨率的特征表示，适合密集视觉任务（这是针对语义分割和目标检测的好处）

* **模型图**

  ![PVT](模型图/PVT.png)

* **参考链接**

  *  [https://blog.csdn.net/m0_45971439/article/details/120495124?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164178066016780261975388%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=164178066016780261975388&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-120495124.first_rank_v2_pc_rank_v29&utm_term=Pyramid+Vision+Transformer%3A+A+Versatile+Backbone+for+Dense+Prediction+without+Convolutions&spm=1018.2226.3001.4187](https://blog.csdn.net/m0_45971439/article/details/120495124?ops_request_misc=%7B%22request%5Fid%22%3A%22164178066016780261975388%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164178066016780261975388&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-120495124.first_rank_v2_pc_rank_v29&utm_term=Pyramid+Vision+Transformer%3A+A+Versatile+Backbone+for+Dense+Prediction+without+Convolutions&spm=1018.2226.3001.4187) 
  *  https://zhuanlan.zhihu.com/p/355867568 

* **代码链接**

  * https://github.com/whai362/PVT







### **6、CoaT (Co-Scale Conv-Attentional Image Transformers)**

* **改进类型**
  * 提高self-attention计算效率，增强表示学习的能力
* **改进原因**
  * self-attention计算量大
* **改进具体方法**
  *  引入了一种允许跨层注意力的co-scale机制，开发了串行块和并行块2种co-scale块，实现了从细到粗、从粗到细和跨尺度的注意力图像建模，增强了表示学习的能力 。
  * 设计Conv-Attention模块在factorized attention模块中实现相对位置嵌入，以此来实现高效的self-attention操作
* **模型图**
  * ![CoaT](模型图/CoaT.png)
* **参考链接**
  *  https://zhuanlan.zhihu.com/p/377337551 
  *  [https://blog.csdn.net/amusi1994/article/details/115879191?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164178469816780261974906%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=164178469816780261974906&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-115879191.first_rank_v2_pc_rank_v29&utm_term=Co-Scale+Conv-Attentional+Image+Transformers&spm=1018.2226.3001.4187](https://blog.csdn.net/amusi1994/article/details/115879191?ops_request_misc=%7B%22request%5Fid%22%3A%22164178469816780261974906%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164178469816780261974906&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-115879191.first_rank_v2_pc_rank_v29&utm_term=Co-Scale+Conv-Attentional+Image+Transformers&spm=1018.2226.3001.4187) 
* **代码链接**
  * https://github.com/mlpc-ucsd/CoaT.





### 7、BoTNet (Bottleneck Transformers)

* 改进类型

  * 降低计算量， 有效处理大分辨率图像。 

* 改进原因

  * 内存和计算量的占用高，导致训练开销比较大。

* 改进具体方法

  *  模型在ResNet最后三个BottleNeck中使用了MHSA替换3x3卷积，属于早期的结合CNN+Transformer的工作，简单来讲Non-Local+Self Attention+BottleNeck = BoTNet 

  *  其核心创新点为 MHSA Block 

    *  归一化这里并没有使用Layer Norm而是采用的Batch Norm 
    *  非线性激活，BoTNet使用了三个非线性激活 
    * 左侧content-position模块引入了二维的位置编码，这是与Transformer最大的区别。

    

* 模型图

  * MHSA模块![1641789291444](模型图/MHSA.png)

* 参考链接

  *  https://zhuanlan.zhihu.com/p/450351712 
  *  [https://blog.csdn.net/qq_19168521/article/details/115228738?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164178786716780271938533%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=164178786716780271938533&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-115228738.first_rank_v2_pc_rank_v29&utm_term=Bottleneck+Transformers+for+Visual+Recognition&spm=1018.2226.3001.4187](https://blog.csdn.net/qq_19168521/article/details/115228738?ops_request_misc=%7B%22request%5Fid%22%3A%22164178786716780271938533%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164178786716780271938533&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-115228738.first_rank_v2_pc_rank_v29&utm_term=Bottleneck+Transformers+for+Visual+Recognition&spm=1018.2226.3001.4187) 

* 代码链接

  * 非官方代码： [leaderj1001/瓶颈变压器：用于视觉识别的瓶颈变压器 (github.com)](https://github.com/leaderj1001/BottleneckTransformers) 





### **8、CoTNet  (Contextual Transformer)**

* 改进类型
  * 得到丰富的上下文关系
* 改进原因
  *  传统的self-attention中，qkv均是通过1*1卷积获得的，成对的qk相乘忽视了相邻的key中蕴含的上下文信息 
* 改进具体方法
  *  提出了一种全新的注意力机制，首先通过3x3的卷积获取融合了上下文信息的key（可以理解为局部上下文信息），再与query进行concat之后，使用两次1x1卷积，然后将其与value相乘得到动态的上下文建模，最后将静态和动态的上下文信息相加进行融合，得到输出。 
* 模型图
  * ![CoTNet](模型图/CoTNet.jpg)
* 参考链接
  *  https://zhuanlan.zhihu.com/p/395607391 
  *  [https://blog.csdn.net/qq_38990652/article/details/120408200?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164178967916780274118277%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=164178967916780274118277&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-120408200.first_rank_v2_pc_rank_v29&utm_term=Contextual+Transformer+Networks+for+Visual+Recognition&spm=1018.2226.3001.4187](https://blog.csdn.net/qq_38990652/article/details/120408200?ops_request_misc=%7B%22request%5Fid%22%3A%22164178967916780274118277%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164178967916780274118277&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-120408200.first_rank_v2_pc_rank_v29&utm_term=Contextual+Transformer+Networks+for+Visual+Recognition&spm=1018.2226.3001.4187) 
* 代码链接
  * https://github.com/JDAI-CV/CoTNet



### 9、Twins （two vision transformer architectures）

* 改进类型

  * 降低计算量，设计高效spatial self-attention

* 改进原因

  * self-attention的计算复杂度很大
  *  PVT 比 Swin-Transformer 的效果略低，其原因应该就在于 PVT 的 ”绝对位置编码” 方法（针对目标检测和语义分割 ）

* 改进具体方法

  *  **Twins-PCPVT** 
    *  ( 建立在PVT和CPVT之上 ) 使用相对位置编码， 最后在分类和其他任务上都取得了性能提升，可以媲美 Swin-Transformer，由于条件位置编码 CPE 支持输入可变长度，使得 Transformer 能够灵活处理不同尺度的输入
  * **Twins-SVT**
    *  提出了 spatially separable self-attention（SSSA），也叫空间可分离注意力，也就是对特征的空间维度进行分组计算各组的注意力，然后再从全局对分组注意力结果进行融合。 SSSA 使用局部-全局注意力交替的机制，可以大幅降低计算量。 

* 模型图

  * Twins-PCPVT

    ![1641791784919](模型图/Twin-PCPVT.png)

  * Twins-SVT

    ![Twin-SVT](模型图/Twin-SVT.png)

* 参考链接

  *  [https://blog.csdn.net/jiaoyangwm/article/details/116303293?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164179055516780255289459%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=164179055516780255289459&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-116303293.first_rank_v2_pc_rank_v29&utm_term=Twins%3A+Revisiting+the+Design+of+Spatial+Attention+in+Vision+Transformers&spm=1018.2226.3001.4187](https://blog.csdn.net/jiaoyangwm/article/details/116303293?ops_request_misc=%7B%22request%5Fid%22%3A%22164179055516780255289459%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164179055516780255289459&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-116303293.first_rank_v2_pc_rank_v29&utm_term=Twins%3A+Revisiting+the+Design+of+Spatial+Attention+in+Vision+Transformers&spm=1018.2226.3001.4187) 
  *  https://zhuanlan.zhihu.com/p/445328841 

* 代码链接

  * https://git.io/Twins.



### 10、XciT  (cross-covariance image transformer)

* 改进类型
  * 减小attention的计算成本（主要针对于目标检测、语义分割，但对分类也有效 ）
* 改进原因
  *  因为传统 Attention 是每个特征间做自相关，因此复杂度与特征数量的平方成正比，这样做会对使大图片显存消耗翻倍增加。 
* 改进具体方法
  *  交叉协方差图像Transformer(XCiT) 建立在 XCA 之上。它结合了传统Transformer的准确性和卷积架构的可扩展性。  互协方差注意力（XCA，cross-covariance attention）在标记数量上具有线性复杂性，并允许对高分辨率图像进行有效处理。 
  *  将做全局自注意力的维度更换一下，将特征间做内积 。简单来说，就是 将 K / Q 两个矩阵映射后做一个标准化，再反过来相乘，以此缩减普通 Attention 模块所需的计算成本。 
* 模型图
  * ![XciT](模型图/XciT.png)
* 参考链接
  *  https://zhuanlan.zhihu.com/p/431953298 
  *  https://zhuanlan.zhihu.com/p/381798434 
  *  https://zhuanlan.zhihu.com/p/382226592 
* 代码链接
  * https://github.com/facebookresearch/xcit



























