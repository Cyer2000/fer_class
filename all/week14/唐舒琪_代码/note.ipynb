{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import things\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import einops\n",
    "\n",
    "from torch import nn\n",
    "from IPython.display import display\n",
    "\n",
    "pil = torchvision.transforms.ToPILImage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying to use einops to split image into patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = torchvision.datasets.MNIST(\"dataset\", download=True)\n",
    "image = mnist.data[0]\n",
    "\n",
    "display(pil(image))\n",
    "display(image.shape)\n",
    "\n",
    "result = einops.rearrange(\n",
    "    image.reshape((28, 28)), \"(pw w) (ph h) -> (pw ph) w h\", pw=2, ph=2\n",
    ")\n",
    "\n",
    "for i in range(4):\n",
    "    display(pil(result[i]))\n",
    "\n",
    "# in the following code we use train_set or test_set instead of mnist.\n",
    "del mnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok let's try other things with einops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "a = torch.rand((1, 28, 28))\n",
    "\n",
    "rearrange(a, \"b (pw w) (ph h) -> b (pw ph) (w h)\", pw=4, ph=4).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got a problem: torch.cat doesn't work the same way as tensor addition. so we need to manually repeat the tensor in other channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor(((1, 1), (2, 2), (3, 3)))\n",
    "a = a.reshape((1, 3, 2))\n",
    "einops.repeat(a, \"() h w -> repeat h w\", repeat=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks good to me\n",
    "\n",
    "now, let's try to create a vision transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but before that, let's try to figure out how pytorch handles tensor addition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([1., 2.])\n",
      "tensor([[2., 4.],\n",
      "        [4., 6.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor(((1, 2), (3, 4), (5, 6)))\n",
    "print(a)\n",
    "b = torch.Tensor((1, 2))\n",
    "print(b)\n",
    "\n",
    "print(a + b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok so it worked as i expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # shape: (batch, 28, 28) == (batch, 4*7, 4*7) -> (batch, 4*4, 7*7)\n",
    "        self.patch_embedding = nn.Sequential(\n",
    "            Rearrange(\"b (pw w) (ph h) -> b (pw ph) (w h)\", pw=4, ph=4),\n",
    "            nn.Linear(7 * 7, 7 * 7),\n",
    "        )\n",
    "        # shape: (batch, 4*4+1, 7*7)\n",
    "        self.class_token = nn.parameter.Parameter(torch.randn((1, 1, 7 * 7)))\n",
    "        # shape: same\n",
    "        self.position_encodings = nn.parameter.Parameter(\n",
    "            torch.randn((1, 4 * 4 + 1, 7 * 7))\n",
    "        )\n",
    "        # the actual transformer, we use the transformer provided by pytorch here.\n",
    "        # d_model is the dimention for the embedding vector\n",
    "        # n_head is the number of heads in the multi head self attention thing\n",
    "        # d_model has to be divisible by n_head\n",
    "        # dim_feedforward is the number of hidden nodes in hidden layer in feedforward block\n",
    "        # batch_first means the first dim of the tensor is batch\n",
    "        # shape: same\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=7 * 7, nhead=7, dim_feedforward=128, batch_first=True\n",
    "            ),\n",
    "            3,\n",
    "        )\n",
    "        # the final MLP\n",
    "        # shape: (batch, 4*4+1, 7*7) -> (batch, 10)\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(7 * 7), nn.Linear(7 * 7, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        batch_size, _, _ = x.shape  \n",
    "        class_token = einops.repeat(\n",
    "            self.class_token,\n",
    "            \"() words features -> repeat words features\",\n",
    "            repeat=batch_size, \n",
    "        )\n",
    "        x = torch.cat((class_token, x), dim=-2)\n",
    "        x += self.position_encodings\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x[:, 0]\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try to use this absolute abomination we've created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_transformer_model = VisionTransformer()\n",
    "vision_transformer_model(torch.rand((1, 28, 28)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, it works. And the shape seems to be alright.\n",
    "\n",
    "now let's try to train it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, we need to load mnist data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    \"dataset\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_set = torchvision.datasets.MNIST(\"dataset\", train=False, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then, let's write the code for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, criterion, epochs, train_loader):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            x = einops.rearrange(x, \"b c h w -> (b c) h w\")\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(\n",
    "                    f\"epoch: {epoch}, i: {batch_idx*len(x)}/{len(train_loader.dataset)}, loss: {loss.item()}\"\n",
    "                )\n",
    "\n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training setup:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 4\n",
    "batch_size = 64\n",
    "optimizer = torch.optim.Adam(vision_transformer_model.parameters(), lr=0.001)\n",
    "# for every step, this scheduler times the lr by gamma\n",
    "# step_size if how many steps before it times the lr by gamma\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's train the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    vision_transformer_model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    criterion,\n",
    "    epochs,\n",
    "    train_loader,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like to be a great success to me\n",
    "\n",
    "( actually, as i was creating this notebook, many things went wrong. But thanks to the magic of _editing_, everything seems to be fine :) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use the model to predict handwritings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a7d19907b29b73a76e90ccf1e029f1f2d2743eb60f7b8c1e389ad5828c35a33"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
