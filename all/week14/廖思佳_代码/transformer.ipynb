{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、准备工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "from torch.autograd import variable\n",
    "\n",
    "seaborn.set_context(context=\"talk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"一个标准的编码-解码架构\"\"\"\n",
    "    #定义一个构造器\n",
    "    def _init_(self,encoder,decoder,src_embd,tgt_embd,generator):\n",
    "        super(EncoderDecoder,self)._init_()\n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        self.src_embd=src_embd\n",
    "        self.tgt_emdb=tgt_embd\n",
    "        self.generator=generator\n",
    "\n",
    "    # take in and process masked src and target sequences\n",
    "    def forward(self,src,tgt,src_mask,tgt_mask):\n",
    "        return self.decode(self.encode(src,src_mask),src_mask,tgt,tgt_mask)\n",
    "\n",
    "    def encode(self,src,src_mask):\n",
    "        return self.encoder(self.src_embd(src),src_mask)\n",
    "\n",
    "    def decode(self,memory,src_mask,tgt,tgt_mask):\n",
    "        return self.decoder(self.tgt_emdb(tgt),memory,src_mask,tgt_mask)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"define standard linear + sofmax generation step\"\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder\n",
    "* 由N=6个相同的层组成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module,N):\n",
    "    \"produce N identical layers\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def _init_(self,layer,N):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.layers=clones(layer,N) # 将一个block复制N次\n",
    "        self.norm=LayerNorm(layer.size)  #进行层连接\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        \"Pass the input (and mask) through each layerin turn\"\n",
    "        for layer in self.layers:\n",
    "            x=layer(x,mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在每个子层之间使用残差连接（Residual Connection）和归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"construct a layernorm module(See citation for deails)\"\n",
    "    def _init_(self,features ,eps=1e-6):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        # torch.ones 返回一个全为1的张量 torch.zeros 返回一个全为标量0的张量\n",
    "        # nn.Parameter 作为nn.Module中可训练的参数使用， 会自动被认为是module的可训练参数，nn.Parameter的对象的requires_grad属性的默认值是True\n",
    "        self.a_2=nn.Parameter(torch.ones(features))\n",
    "        self.b_2=nn.Parameter(torch.zeros(features))\n",
    "        self.eps=eps\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(-1,keepdim=True) # 求均值\n",
    "        std=x.std(-1,keepdim=True) # 计算标准差\n",
    "        return self.a_2*(x-mean)/(std + self.eps)+self.b_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个子层的输出为[公式]，其中[公式]是由子层自动实现的函数。我们在每个子层的输出上使用Dropout，然后将其添加到下一子层的输入并进行归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self,size,dropout):\n",
    "        super(SublayerConnection,self)._init_()\n",
    "        self.norm=LayerNorm(size)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,sublayer):\n",
    "       \"Apply residual connection to any sublayer with the same size.\"\n",
    "       return x+self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每层都有两个子层组成。第一个子层实现了“多头”的 Self-attention，第二个子层则是一个简单的Position-wise的全连接前馈网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def _init_(self,size,self_attn,)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be203ce0b3afc4f5c37fbac412025d7ed1d67cabe9dd00b1fc8774c6d6d19d70"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
