{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import things\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mPython 3.7.11 64-bit ('py37': conda) 需要安装 ipykernel。\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n py37 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import einops\n",
    "\n",
    "from torch import nn\n",
    "from IPython.display import display\n",
    "\n",
    "pil = torchvision.transforms.ToPILImage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying to use einops to split image into patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2423E671FA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAAAAAA6I3INAAAASElEQVR4nGNgoBpgROYw88O5cmxWNgLBMJ7hu79///79ywTlPnyLakjAnOy/f88i+HyMs/5GscC5nxg+MqQgq+fe9xdFv/JHAA2UFDVP2O4nAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=14x14 at 0x2423E584370>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAAAAAA6I3INAAAAVElEQVR4nGNgoB4QEqrrWC+17P/3egYGBgaGv3///n245u+nww4MDAwMjH8ZGP4lfWV49v4mAwMDA8Oxbd8/IunlZtCehdtkJlSeC6rswwU5jAgeACtkGe3wb2RDAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=14x14 at 0x2423E6961C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAAAAAA6I3INAAAASklEQVR4nGNgQAaBDNQC6lOQOBJFd//COeJOV//+hXGFVt/++/fv38MMDAwMDOZrHv39+/fvl1ZuFgYGBobAQAaG65v/9nwgzWYAzKEZy8WlxqcAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=14x14 at 0x2423E6961C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAAAAAA6I3INAAAATUlEQVR4nGP48PdvuSQDHOju+vt3mjSCLxD75+9uBiTw8+9PByiTRS/ElIXh2iGY1NO/f//+2gZX+ffv35N+CI1/jwUyIZkTwMlAGwAAoLkbvXiHWfYAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=14x14 at 0x2423E6961C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist = torchvision.datasets.MNIST(\"dataset\", download=True)\n",
    "image = mnist.data[0]\n",
    "\n",
    "display(pil(image))\n",
    "display(image.shape)\n",
    "\n",
    "result = einops.rearrange(\n",
    "    image.reshape((28, 28)), \"(pw w) (ph h) -> (pw ph) w h\", pw=2, ph=2\n",
    ")\n",
    "\n",
    "for i in range(4):\n",
    "    display(pil(result[i]))\n",
    "\n",
    "# in the following code we use train_set or test_set instead of mnist.\n",
    "del mnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok let's try other things with einops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 49])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "a = torch.rand((1, 28, 28))\n",
    "\n",
    "rearrange(a, \"b (pw w) (ph h) -> b (pw ph) (w h)\", pw=4, ph=4).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got a problem: torch.cat doesn't work the same way as tensor addition. so we need to manually repeat the tensor in other channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [2., 2.],\n",
       "         [3., 3.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [2., 2.],\n",
       "         [3., 3.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [2., 2.],\n",
       "         [3., 3.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor(((1, 1), (2, 2), (3, 3)))\n",
    "a = a.reshape((1, 3, 2))\n",
    "einops.repeat(a, \"() h w -> repeat h w\", repeat=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks good to me\n",
    "\n",
    "now, let's try to create a vision transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but before that, let's try to figure out how pytorch handles tensor addition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([1., 2.])\n",
      "tensor([[2., 4.],\n",
      "        [4., 6.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor(((1, 2), (3, 4), (5, 6)))\n",
    "print(a)\n",
    "b = torch.Tensor((1, 2))\n",
    "print(b)\n",
    "\n",
    "print(a + b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok so it worked as i expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # shape: (batch, 28, 28) == (batch, 4*7, 4*7) -> (batch, 4*4, 7*7)\n",
    "        self.patch_embedding = nn.Sequential(\n",
    "            Rearrange(\"b (pw w) (ph h) -> b (pw ph) (w h)\", pw=4, ph=4),\n",
    "            nn.Linear(7 * 7, 7 * 7),\n",
    "        )\n",
    "        # shape: (batch, 4*4+1, 7*7)\n",
    "        self.class_token = nn.parameter.Parameter(torch.randn((1, 1, 7 * 7)))\n",
    "        # shape: same\n",
    "        self.position_encodings = nn.parameter.Parameter(\n",
    "            torch.randn((1, 4 * 4 + 1, 7 * 7))\n",
    "        )\n",
    "        # the actual transformer, we use the transformer provided by pytorch here.\n",
    "        # d_model is the dimention for the embedding vector\n",
    "        # n_head is the number of heads in the multi head self attention thing\n",
    "        # d_model has to be divisible by n_head\n",
    "        # dim_feedforward is the number of hidden nodes in hidden layer in feedforward block\n",
    "        # batch_first means the first dim of the tensor is batch\n",
    "        # shape: same\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=7 * 7, nhead=7, dim_feedforward=128, batch_first=True\n",
    "            ),\n",
    "            3,\n",
    "        )\n",
    "        # the final MLP\n",
    "        # shape: (batch, 4*4+1, 7*7) -> (batch, 10)\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(7 * 7), nn.Linear(7 * 7, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        batch_size, _, _ = x.shape\n",
    "        class_token = einops.repeat(\n",
    "            self.class_token,\n",
    "            \"() words features -> repeat words features\",\n",
    "            repeat=batch_size,\n",
    "        )\n",
    "        x = torch.cat((class_token, x), dim=-2)\n",
    "        x += self.position_encodings\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x[:, 0]\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try to use this absolute abomination we've created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0702, -0.4120, -0.4161,  0.0617,  0.1407, -0.6397,  0.7642, -0.6262,\n",
       "         -0.5885, -0.1532]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_transformer_model = VisionTransformer()\n",
    "vision_transformer_model(torch.rand((1, 28, 28)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, it works. And the shape seems to be alright.\n",
    "\n",
    "now let's try to train it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, we need to load mnist data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to dataset\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset\\MNIST\\raw\\train-images-idx3-ubyte.gz to dataset\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to dataset\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset\\MNIST\\raw\\train-labels-idx1-ubyte.gz to dataset\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to dataset\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to dataset\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to dataset\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to dataset\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    \"dataset\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_set = torchvision.datasets.MNIST(\"dataset\", train=False, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then, let's write the code for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, criterion, epochs, train_loader):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            x = einops.rearrange(x, \"b c h w -> (b c) h w\")\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(\n",
    "                    f\"epoch: {epoch}, i: {batch_idx*len(x)}/{len(train_loader.dataset)}, loss: {loss.item()}\"\n",
    "                )\n",
    "\n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training setup:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 4\n",
    "batch_size = 64\n",
    "optimizer = torch.optim.Adam(vision_transformer_model.parameters(), lr=0.001)\n",
    "# for every step, this scheduler times the lr by gamma\n",
    "# step_size if how many steps before it times the lr by gamma\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's train the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, i: 0/60000, loss: 2.347409963607788\n",
      "epoch: 0, i: 640/60000, loss: 2.2308762073516846\n",
      "epoch: 0, i: 1280/60000, loss: 2.1063649654388428\n",
      "epoch: 0, i: 1920/60000, loss: 1.6644877195358276\n",
      "epoch: 0, i: 2560/60000, loss: 1.372070074081421\n",
      "epoch: 0, i: 3200/60000, loss: 1.1328141689300537\n",
      "epoch: 0, i: 3840/60000, loss: 0.9578725695610046\n",
      "epoch: 0, i: 4480/60000, loss: 0.8262578248977661\n",
      "epoch: 0, i: 5120/60000, loss: 0.975385308265686\n",
      "epoch: 0, i: 5760/60000, loss: 0.660971462726593\n",
      "epoch: 0, i: 6400/60000, loss: 0.6545061469078064\n",
      "epoch: 0, i: 7040/60000, loss: 0.5432608127593994\n",
      "epoch: 0, i: 7680/60000, loss: 0.5247681140899658\n",
      "epoch: 0, i: 8320/60000, loss: 0.4616728127002716\n",
      "epoch: 0, i: 8960/60000, loss: 0.4974525570869446\n",
      "epoch: 0, i: 9600/60000, loss: 0.5619866251945496\n",
      "epoch: 0, i: 10240/60000, loss: 0.819754958152771\n",
      "epoch: 0, i: 10880/60000, loss: 0.43658339977264404\n",
      "epoch: 0, i: 11520/60000, loss: 0.6873295307159424\n",
      "epoch: 0, i: 12160/60000, loss: 0.43437400460243225\n",
      "epoch: 0, i: 12800/60000, loss: 0.39025259017944336\n",
      "epoch: 0, i: 13440/60000, loss: 0.31740060448646545\n",
      "epoch: 0, i: 14080/60000, loss: 0.4845370650291443\n",
      "epoch: 0, i: 14720/60000, loss: 0.6709620952606201\n",
      "epoch: 0, i: 15360/60000, loss: 0.39946043491363525\n",
      "epoch: 0, i: 16000/60000, loss: 0.6988015174865723\n",
      "epoch: 0, i: 16640/60000, loss: 0.6068139672279358\n",
      "epoch: 0, i: 17280/60000, loss: 0.35761404037475586\n",
      "epoch: 0, i: 17920/60000, loss: 0.21380457282066345\n",
      "epoch: 0, i: 18560/60000, loss: 0.30868563055992126\n",
      "epoch: 0, i: 19200/60000, loss: 0.42138752341270447\n",
      "epoch: 0, i: 19840/60000, loss: 0.23501136898994446\n",
      "epoch: 0, i: 20480/60000, loss: 0.21988515555858612\n",
      "epoch: 0, i: 21120/60000, loss: 0.25214719772338867\n",
      "epoch: 0, i: 21760/60000, loss: 0.15994568169116974\n",
      "epoch: 0, i: 22400/60000, loss: 0.24954229593276978\n",
      "epoch: 0, i: 23040/60000, loss: 0.4369667172431946\n",
      "epoch: 0, i: 23680/60000, loss: 0.45155131816864014\n",
      "epoch: 0, i: 24320/60000, loss: 0.20768554508686066\n",
      "epoch: 0, i: 24960/60000, loss: 0.31029027700424194\n",
      "epoch: 0, i: 25600/60000, loss: 0.20119813084602356\n",
      "epoch: 0, i: 26240/60000, loss: 0.27951881289482117\n",
      "epoch: 0, i: 26880/60000, loss: 0.23790930211544037\n",
      "epoch: 0, i: 27520/60000, loss: 0.2056615650653839\n",
      "epoch: 0, i: 28160/60000, loss: 0.21679595112800598\n",
      "epoch: 0, i: 28800/60000, loss: 0.2247699350118637\n",
      "epoch: 0, i: 29440/60000, loss: 0.23931729793548584\n",
      "epoch: 0, i: 30080/60000, loss: 0.40423688292503357\n",
      "epoch: 0, i: 30720/60000, loss: 0.2947724163532257\n",
      "epoch: 0, i: 31360/60000, loss: 0.3389212489128113\n",
      "epoch: 0, i: 32000/60000, loss: 0.2866861820220947\n",
      "epoch: 0, i: 32640/60000, loss: 0.25262516736984253\n",
      "epoch: 0, i: 33280/60000, loss: 0.24338620901107788\n",
      "epoch: 0, i: 33920/60000, loss: 0.18123196065425873\n",
      "epoch: 0, i: 34560/60000, loss: 0.23410992324352264\n",
      "epoch: 0, i: 35200/60000, loss: 0.23958566784858704\n",
      "epoch: 0, i: 35840/60000, loss: 0.2678765654563904\n",
      "epoch: 0, i: 36480/60000, loss: 0.14864090085029602\n",
      "epoch: 0, i: 37120/60000, loss: 0.3976374566555023\n",
      "epoch: 0, i: 37760/60000, loss: 0.2505382299423218\n",
      "epoch: 0, i: 38400/60000, loss: 0.24038666486740112\n",
      "epoch: 0, i: 39040/60000, loss: 0.07883477210998535\n",
      "epoch: 0, i: 39680/60000, loss: 0.17040471732616425\n",
      "epoch: 0, i: 40320/60000, loss: 0.09807787835597992\n",
      "epoch: 0, i: 40960/60000, loss: 0.40951642394065857\n",
      "epoch: 0, i: 41600/60000, loss: 0.21235306560993195\n",
      "epoch: 0, i: 42240/60000, loss: 0.18211400508880615\n",
      "epoch: 0, i: 42880/60000, loss: 0.4268750250339508\n",
      "epoch: 0, i: 43520/60000, loss: 0.1499943882226944\n",
      "epoch: 0, i: 44160/60000, loss: 0.27330949902534485\n",
      "epoch: 0, i: 44800/60000, loss: 0.3703700006008148\n",
      "epoch: 0, i: 45440/60000, loss: 0.41589587926864624\n",
      "epoch: 0, i: 46080/60000, loss: 0.32816988229751587\n",
      "epoch: 0, i: 46720/60000, loss: 0.31820085644721985\n",
      "epoch: 0, i: 47360/60000, loss: 0.34358227252960205\n",
      "epoch: 0, i: 48000/60000, loss: 0.2367860972881317\n",
      "epoch: 0, i: 48640/60000, loss: 0.16063113510608673\n",
      "epoch: 0, i: 49280/60000, loss: 0.1264689862728119\n",
      "epoch: 0, i: 49920/60000, loss: 0.1963081657886505\n",
      "epoch: 0, i: 50560/60000, loss: 0.22686436772346497\n",
      "epoch: 0, i: 51200/60000, loss: 0.3637060523033142\n",
      "epoch: 0, i: 51840/60000, loss: 0.17444710433483124\n",
      "epoch: 0, i: 52480/60000, loss: 0.08826502412557602\n",
      "epoch: 0, i: 53120/60000, loss: 0.2940919101238251\n",
      "epoch: 0, i: 53760/60000, loss: 0.09032779186964035\n",
      "epoch: 0, i: 54400/60000, loss: 0.1511324644088745\n",
      "epoch: 0, i: 55040/60000, loss: 0.1472872644662857\n",
      "epoch: 0, i: 55680/60000, loss: 0.22550925612449646\n",
      "epoch: 0, i: 56320/60000, loss: 0.19666309654712677\n",
      "epoch: 0, i: 56960/60000, loss: 0.14165841042995453\n",
      "epoch: 0, i: 57600/60000, loss: 0.27760744094848633\n",
      "epoch: 0, i: 58240/60000, loss: 0.08359356224536896\n",
      "epoch: 0, i: 58880/60000, loss: 0.07816105335950851\n",
      "epoch: 0, i: 59520/60000, loss: 0.049126338213682175\n",
      "epoch: 1, i: 0/60000, loss: 0.16396865248680115\n",
      "epoch: 1, i: 640/60000, loss: 0.146816223859787\n",
      "epoch: 1, i: 1280/60000, loss: 0.133257195353508\n",
      "epoch: 1, i: 1920/60000, loss: 0.2183443009853363\n",
      "epoch: 1, i: 2560/60000, loss: 0.08982986211776733\n",
      "epoch: 1, i: 3200/60000, loss: 0.19368663430213928\n",
      "epoch: 1, i: 3840/60000, loss: 0.09041225910186768\n",
      "epoch: 1, i: 4480/60000, loss: 0.16700617969036102\n",
      "epoch: 1, i: 5120/60000, loss: 0.3357411026954651\n",
      "epoch: 1, i: 5760/60000, loss: 0.15072013437747955\n",
      "epoch: 1, i: 6400/60000, loss: 0.20923593640327454\n",
      "epoch: 1, i: 7040/60000, loss: 0.17427808046340942\n",
      "epoch: 1, i: 7680/60000, loss: 0.2374560534954071\n",
      "epoch: 1, i: 8320/60000, loss: 0.07550907135009766\n",
      "epoch: 1, i: 8960/60000, loss: 0.1281883716583252\n",
      "epoch: 1, i: 9600/60000, loss: 0.21109090745449066\n",
      "epoch: 1, i: 10240/60000, loss: 0.6055009961128235\n",
      "epoch: 1, i: 10880/60000, loss: 0.1030803918838501\n",
      "epoch: 1, i: 11520/60000, loss: 0.2810560166835785\n",
      "epoch: 1, i: 12160/60000, loss: 0.1080985888838768\n",
      "epoch: 1, i: 12800/60000, loss: 0.17017138004302979\n",
      "epoch: 1, i: 13440/60000, loss: 0.1279321014881134\n",
      "epoch: 1, i: 14080/60000, loss: 0.08107511699199677\n",
      "epoch: 1, i: 14720/60000, loss: 0.30666425824165344\n",
      "epoch: 1, i: 15360/60000, loss: 0.10088899731636047\n",
      "epoch: 1, i: 16000/60000, loss: 0.25454986095428467\n",
      "epoch: 1, i: 16640/60000, loss: 0.19290676712989807\n",
      "epoch: 1, i: 17280/60000, loss: 0.04817470535635948\n",
      "epoch: 1, i: 17920/60000, loss: 0.12399471551179886\n",
      "epoch: 1, i: 18560/60000, loss: 0.1659337431192398\n",
      "epoch: 1, i: 19200/60000, loss: 0.14187251031398773\n",
      "epoch: 1, i: 19840/60000, loss: 0.12678730487823486\n",
      "epoch: 1, i: 20480/60000, loss: 0.10321144759654999\n",
      "epoch: 1, i: 21120/60000, loss: 0.18626271188259125\n",
      "epoch: 1, i: 21760/60000, loss: 0.0914958044886589\n",
      "epoch: 1, i: 22400/60000, loss: 0.0471017025411129\n",
      "epoch: 1, i: 23040/60000, loss: 0.1525849848985672\n",
      "epoch: 1, i: 23680/60000, loss: 0.23440153896808624\n",
      "epoch: 1, i: 24320/60000, loss: 0.12564052641391754\n",
      "epoch: 1, i: 24960/60000, loss: 0.11527667194604874\n",
      "epoch: 1, i: 25600/60000, loss: 0.07755744457244873\n",
      "epoch: 1, i: 26240/60000, loss: 0.11015519499778748\n",
      "epoch: 1, i: 26880/60000, loss: 0.1664862036705017\n",
      "epoch: 1, i: 27520/60000, loss: 0.17222633957862854\n",
      "epoch: 1, i: 28160/60000, loss: 0.13261575996875763\n",
      "epoch: 1, i: 28800/60000, loss: 0.0900166854262352\n",
      "epoch: 1, i: 29440/60000, loss: 0.12032771855592728\n",
      "epoch: 1, i: 30080/60000, loss: 0.2167212963104248\n",
      "epoch: 1, i: 30720/60000, loss: 0.3352491855621338\n",
      "epoch: 1, i: 31360/60000, loss: 0.22679321467876434\n",
      "epoch: 1, i: 32000/60000, loss: 0.2108190655708313\n",
      "epoch: 1, i: 32640/60000, loss: 0.07677323371171951\n",
      "epoch: 1, i: 33280/60000, loss: 0.1545911729335785\n",
      "epoch: 1, i: 33920/60000, loss: 0.018407903611660004\n",
      "epoch: 1, i: 34560/60000, loss: 0.07068489491939545\n",
      "epoch: 1, i: 35200/60000, loss: 0.2062978893518448\n",
      "epoch: 1, i: 35840/60000, loss: 0.14371494948863983\n",
      "epoch: 1, i: 36480/60000, loss: 0.05413948744535446\n",
      "epoch: 1, i: 37120/60000, loss: 0.3415299952030182\n",
      "epoch: 1, i: 37760/60000, loss: 0.12661056220531464\n",
      "epoch: 1, i: 38400/60000, loss: 0.10879980772733688\n",
      "epoch: 1, i: 39040/60000, loss: 0.013886220753192902\n",
      "epoch: 1, i: 39680/60000, loss: 0.08892921358346939\n",
      "epoch: 1, i: 40320/60000, loss: 0.09983935207128525\n",
      "epoch: 1, i: 40960/60000, loss: 0.22830747067928314\n",
      "epoch: 1, i: 41600/60000, loss: 0.11016795039176941\n",
      "epoch: 1, i: 42240/60000, loss: 0.04927762970328331\n",
      "epoch: 1, i: 42880/60000, loss: 0.22101880609989166\n",
      "epoch: 1, i: 43520/60000, loss: 0.16313111782073975\n",
      "epoch: 1, i: 44160/60000, loss: 0.13046810030937195\n",
      "epoch: 1, i: 44800/60000, loss: 0.14276337623596191\n",
      "epoch: 1, i: 45440/60000, loss: 0.21303865313529968\n",
      "epoch: 1, i: 46080/60000, loss: 0.18865932524204254\n",
      "epoch: 1, i: 46720/60000, loss: 0.2486143261194229\n",
      "epoch: 1, i: 47360/60000, loss: 0.19785048067569733\n",
      "epoch: 1, i: 48000/60000, loss: 0.09606214612722397\n",
      "epoch: 1, i: 48640/60000, loss: 0.07145293056964874\n",
      "epoch: 1, i: 49280/60000, loss: 0.11509955674409866\n",
      "epoch: 1, i: 49920/60000, loss: 0.1300610899925232\n",
      "epoch: 1, i: 50560/60000, loss: 0.1493290364742279\n",
      "epoch: 1, i: 51200/60000, loss: 0.25365397334098816\n",
      "epoch: 1, i: 51840/60000, loss: 0.09126371890306473\n",
      "epoch: 1, i: 52480/60000, loss: 0.07833188027143478\n",
      "epoch: 1, i: 53120/60000, loss: 0.19458870589733124\n",
      "epoch: 1, i: 53760/60000, loss: 0.07047437131404877\n",
      "epoch: 1, i: 54400/60000, loss: 0.1057979166507721\n",
      "epoch: 1, i: 55040/60000, loss: 0.05605560541152954\n",
      "epoch: 1, i: 55680/60000, loss: 0.10913623124361038\n",
      "epoch: 1, i: 56320/60000, loss: 0.16307224333286285\n",
      "epoch: 1, i: 56960/60000, loss: 0.15703514218330383\n",
      "epoch: 1, i: 57600/60000, loss: 0.2214507907629013\n",
      "epoch: 1, i: 58240/60000, loss: 0.06706150621175766\n",
      "epoch: 1, i: 58880/60000, loss: 0.06482579559087753\n",
      "epoch: 1, i: 59520/60000, loss: 0.013618139550089836\n",
      "epoch: 2, i: 0/60000, loss: 0.11172080039978027\n",
      "epoch: 2, i: 640/60000, loss: 0.09360185265541077\n",
      "epoch: 2, i: 1280/60000, loss: 0.06467973440885544\n",
      "epoch: 2, i: 1920/60000, loss: 0.15371812880039215\n",
      "epoch: 2, i: 2560/60000, loss: 0.12144719064235687\n",
      "epoch: 2, i: 3200/60000, loss: 0.11612533032894135\n",
      "epoch: 2, i: 3840/60000, loss: 0.10968906432390213\n",
      "epoch: 2, i: 4480/60000, loss: 0.07090914994478226\n",
      "epoch: 2, i: 5120/60000, loss: 0.18259669840335846\n",
      "epoch: 2, i: 5760/60000, loss: 0.12040193378925323\n",
      "epoch: 2, i: 6400/60000, loss: 0.1681237369775772\n",
      "epoch: 2, i: 7040/60000, loss: 0.15251168608665466\n",
      "epoch: 2, i: 7680/60000, loss: 0.12364685535430908\n",
      "epoch: 2, i: 8320/60000, loss: 0.04817758873105049\n",
      "epoch: 2, i: 8960/60000, loss: 0.11731021106243134\n",
      "epoch: 2, i: 9600/60000, loss: 0.2284977287054062\n",
      "epoch: 2, i: 10240/60000, loss: 0.3666500151157379\n",
      "epoch: 2, i: 10880/60000, loss: 0.055635008960962296\n",
      "epoch: 2, i: 11520/60000, loss: 0.32373279333114624\n",
      "epoch: 2, i: 12160/60000, loss: 0.06290518492460251\n",
      "epoch: 2, i: 12800/60000, loss: 0.10968314856290817\n",
      "epoch: 2, i: 13440/60000, loss: 0.10344655811786652\n",
      "epoch: 2, i: 14080/60000, loss: 0.05880552902817726\n",
      "epoch: 2, i: 14720/60000, loss: 0.4222823679447174\n",
      "epoch: 2, i: 15360/60000, loss: 0.15463830530643463\n",
      "epoch: 2, i: 16000/60000, loss: 0.2337360382080078\n",
      "epoch: 2, i: 16640/60000, loss: 0.09697894006967545\n",
      "epoch: 2, i: 17280/60000, loss: 0.02057458832859993\n",
      "epoch: 2, i: 17920/60000, loss: 0.08868980407714844\n",
      "epoch: 2, i: 18560/60000, loss: 0.09340589493513107\n",
      "epoch: 2, i: 19200/60000, loss: 0.10824085772037506\n",
      "epoch: 2, i: 19840/60000, loss: 0.06142733618617058\n",
      "epoch: 2, i: 20480/60000, loss: 0.0714864581823349\n",
      "epoch: 2, i: 21120/60000, loss: 0.061708513647317886\n",
      "epoch: 2, i: 21760/60000, loss: 0.03573288023471832\n",
      "epoch: 2, i: 22400/60000, loss: 0.06325624138116837\n",
      "epoch: 2, i: 23040/60000, loss: 0.17338700592517853\n",
      "epoch: 2, i: 23680/60000, loss: 0.23929592967033386\n",
      "epoch: 2, i: 24320/60000, loss: 0.04900399595499039\n",
      "epoch: 2, i: 24960/60000, loss: 0.03345493599772453\n",
      "epoch: 2, i: 25600/60000, loss: 0.09428736567497253\n",
      "epoch: 2, i: 26240/60000, loss: 0.09437523037195206\n",
      "epoch: 2, i: 26880/60000, loss: 0.17721806466579437\n",
      "epoch: 2, i: 27520/60000, loss: 0.16118545830249786\n",
      "epoch: 2, i: 28160/60000, loss: 0.09655200690031052\n",
      "epoch: 2, i: 28800/60000, loss: 0.0448741689324379\n",
      "epoch: 2, i: 29440/60000, loss: 0.06506302207708359\n",
      "epoch: 2, i: 30080/60000, loss: 0.17998598515987396\n",
      "epoch: 2, i: 30720/60000, loss: 0.09562858194112778\n",
      "epoch: 2, i: 31360/60000, loss: 0.09594239294528961\n",
      "epoch: 2, i: 32000/60000, loss: 0.09520074725151062\n",
      "epoch: 2, i: 32640/60000, loss: 0.1152222603559494\n",
      "epoch: 2, i: 33280/60000, loss: 0.10912024229764938\n",
      "epoch: 2, i: 33920/60000, loss: 0.03540697693824768\n",
      "epoch: 2, i: 34560/60000, loss: 0.03154333308339119\n",
      "epoch: 2, i: 35200/60000, loss: 0.1524907350540161\n",
      "epoch: 2, i: 35840/60000, loss: 0.13545438647270203\n",
      "epoch: 2, i: 36480/60000, loss: 0.11211802810430527\n",
      "epoch: 2, i: 37120/60000, loss: 0.13966862857341766\n",
      "epoch: 2, i: 37760/60000, loss: 0.17714785039424896\n",
      "epoch: 2, i: 38400/60000, loss: 0.054525766521692276\n",
      "epoch: 2, i: 39040/60000, loss: 0.0413699597120285\n",
      "epoch: 2, i: 39680/60000, loss: 0.12088824808597565\n",
      "epoch: 2, i: 40320/60000, loss: 0.06486493349075317\n",
      "epoch: 2, i: 40960/60000, loss: 0.1816156655550003\n",
      "epoch: 2, i: 41600/60000, loss: 0.07449636608362198\n",
      "epoch: 2, i: 42240/60000, loss: 0.07740923017263412\n",
      "epoch: 2, i: 42880/60000, loss: 0.13358236849308014\n",
      "epoch: 2, i: 43520/60000, loss: 0.0922442153096199\n",
      "epoch: 2, i: 44160/60000, loss: 0.15999290347099304\n",
      "epoch: 2, i: 44800/60000, loss: 0.16822274029254913\n",
      "epoch: 2, i: 45440/60000, loss: 0.24671559035778046\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7012/2923816994.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mvision_transformer_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7012/3407704868.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, scheduler, criterion, epochs, train_loader)\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    vision_transformer_model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    criterion,\n",
    "    epochs,\n",
    "    train_loader,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like to be a great success to me\n",
    "\n",
    "( actually, as i was creating this notebook, many things went wrong. But thanks to the magic of _editing_, everything seems to be fine :) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use the model to predict handwritings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a7d19907b29b73a76e90ccf1e029f1f2d2743eb60f7b8c1e389ad5828c35a33"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit ('pytorch': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
